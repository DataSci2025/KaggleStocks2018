import pandas as pd
from datetime import datetime 


def common_elements(list1, list2):
    outlist=[]
    for x in list1:
        for y in list2:            
            date_format = "%Y-%m-%d"
            a = datetime.strptime(x, date_format)
            b = datetime.strptime(y, date_format)
            dif = a-b
            if dif.days < 14 and dif.days > -14:
                if x not in outlist:
                    outlist.append(x)
                if y not in outlist:
                    outlist.append(y)
    return outlist

# Load tickers from the Symbol column of the Kaggle Dataset
df = pd.read_csv(r"D:\AnonUser\Documents\financials.csv")
print(df.head())

df1 = df[df.isna().any(axis=1)]


#Drop any columns with nulls to ensure data integrity
df_adj = df.dropna()

#Test for unique Ticker/Symbol - is it a Primary Key?
uniquetest = df['Symbol'].is_unique
print(uniquetest)

#Read the yfinance data from Step 1
df2 = pd.read_csv(r"C:\Users\AnonUser\Documents\SP500_2018.csv")
datelist = []
olddatelist = []
breakerlist =[]
started = False

#The below code does the following:
# If yfinance data is blank, skip
# Otherwise, look for dates where the price from yfinance matches the price in the Kaggle dataset
# This test is: the Kaggle High value should represent the Max price of a given day/period, and the Low the min
# allowing for some error (2%), return days in yfinance where the price is between the two
# If this day is within 14 days of previous days recorded in the loop (using the common_elements function above) then keep both days in the list
# This is a bit of an imperfect bootstrap method of working, but  ensures that within an error of < 2 weeks dates can be matched across stocks
# The objective is to see if there is a common date on which the Kaggle data was pulled; this is not mentioned in the Kaggle dataset

for index, row in df_adj.iterrows():

    x=row['Symbol']
    y=row['Price']
    print(x)
    

    filtered_df = df2[df2['Ticker'] == x]
    print(filtered_df.head())
    
    if len(filtered_df) > 0:
    
        datelist = []
        for index2, row2 in filtered_df.iterrows():
            
            if row2['High'] >= y*0.98 and row2['Low'] <= y*1.02:
                 datelist.append(row2['Date'])
        if len(datelist) > 0:
            if started == False:
                started = True
                olddatelist=datelist
                initialdatelist = olddatelist
               
            else:
                refreshdatelist = common_elements(olddatelist,datelist)
                if len(refreshdatelist) == 0 and len(olddatelist) > 0:
         
                    breakerlist.append(x)
                else:
                    
                    olddatelist = refreshdatelist
            
            print(olddatelist)
        
print(initialdatelist)       
print(olddatelist)

#In an earlier iteration of the code, the presence of a Stock without matching dates was meant to literally break the loop
#This was revised - instead, stocks with no matching dates were logged and then the loop continues. These are then dropped.
print(breakerlist)    
#The print ensures these can be noted in the Appendix

df_adj_valid = df_adj[~df_adj['Symbol'].isin(breakerlist)]

print(df_adj_valid)

#Renames columns - in particular, the 52_Week_Low and 52_Week_High were the wrong way around
df_adj_valid=df_adj_valid.rename(columns={"52 Week Low":"52_Week_High", "52 Week High":"52_Week_Low", "Dividend Yield":"Dividend_Yield"})

#SEC Filings was a simple link to the Financial Reports for the company, and was not needed
df_adj_valid = df_adj_valid.drop('SEC Filings', axis=1)

#Saves with stocks with any field null dropped, and stocks where yfinance prices are incompatable with the detected Jan/Feb 2018 timescale dropped
df_adj_valid.to_csv(r"D:\AnonUser\Documents\UniStocks\AdjustedStocksList.csv",index=False)




#Dates output = ['2018-02-16', '2018-02-06', '2018-02-07', '2018-02-08', '2018-02-09', '2018-02-12', '2018-02-14', '2018-02-05', '2018-02-13', '2018-02-15', '2018-02-20', '2018-02-21', '2018-02-22', '2018-02-23', '2018-02-26', '2018-01-29', '2018-01-16', '2018-01-03', '2018-01-04', '2018-01-05', '2017-12-21', '2017-12-22', '2017-12-26', '2017-12-27', '2017-12-28', '2017-12-29', '2018-01-02', '2018-01-08', '2018-01-09', '2018-01-30', '2018-01-31', '2018-02-01', '2018-02-02']
#This indicates a likely January/February 2018 date
